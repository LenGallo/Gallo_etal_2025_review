{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ea160e-85ad-4bf3-b004-1a4a67726241",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b9c465-f542-47e0-bbe8-3cbfd155f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrog/opt/anaconda3/envs/ml_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import shap\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888b5c7-5c0c-40a0-8fec-73ded546ac6a",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n",
    "We start preprocessing the geochemical and magnetic susceptibility datasets\n",
    "for further analysis. \n",
    "\n",
    "The following steps include loading the susceptibility and geochemistriy datasets, merge on the sample \n",
    "and cleaning the data.\n",
    "\n",
    "We first get the raw data and resolve `NaN` values. In this first approach we get rid of the columns with `NaN`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf32791c-d00f-4386-bffe-5ba332500853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset contains geochemical measurements for different samples.\n",
    "df_xrf_corrected = pd.read_csv('../Data/TG_xrf_corrected.csv')\n",
    "\n",
    "# This dataset includes magnetic susceptibility measurements for the samples.\n",
    "df_susc = pd.read_csv('../Data/TG_Bulk_Susceptibility_all.csv')\n",
    "\n",
    "# We perform a left join to retain all XRF data and add susceptibility values where available.\n",
    "df_all = pd.merge(df_xrf_corrected, df_susc[['sample','X_fv']], on='sample', how='left')\n",
    "\n",
    "### Remove samples with missing susceptibility values\n",
    "df_all = df_all[~df_all['X_fv'].isna()]\n",
    "\n",
    "### Drop categorical identifiers\n",
    "# The 'sample' and 'section' columns are metadata and are not needed for numerical analysis.\n",
    "df_all = df_all.drop(columns = ['sample', 'section'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef32e5-f184-4475-8e9b-11527f411b48",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "Before performing further analysis, we check for `Nan` values in the dataframe (<LOD; below limit of detection). If any missing values exist, we replace them with 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01d778-4cc9-49b8-8f7f-736b512f9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = df_all.isna().sum()\n",
    "print(\"Number of nans: \", np.sum(nan_counts))\n",
    "\n",
    "# Fill NaNs with 0.0 and verify that no missing values remain\n",
    "df = df_all.fillna(0.0)\n",
    "assert np.sum(df.isna().sum()) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325e60d-68b9-4a5d-826c-b503f9d2f793",
   "metadata": {},
   "source": [
    "### Compute Ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44273ea1-ba7a-4232-9d0d-0f3c7ae7c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ratios = True\n",
    "\n",
    "if add_ratios:\n",
    "    \n",
    "    df['Rb/Sr'] = df['Rb'] /  df['Sr']\n",
    "    df['Ca/Al'] = df['Ca'] /  df['Al2O3']\n",
    "    df['Fe/S']  = df['Fe'] / df['S']\n",
    "    df['Si/Al'] = df['SiO2'] / df['Al2O3']\n",
    "    df['K/Al']  = df['K2O'] / df['Al2O3']\n",
    "    df['K/Rb']  = df['K2O'] / df['Rb']\n",
    "    df['Rb/Al'] = df['Rb'] / df['Al2O3']\n",
    "    df['Si/Al'] = df['SiO2'] / df['Ca']\n",
    "    df['Ti/Al'] = df['Ti'] / df['Al2O3'] # https://cp.copernicus.org/articles/20/415/2024/cp-20-415-2024.pdf\n",
    "\n",
    "    # Replace nans again but now by infinity\n",
    "    print(\"Total number of NaNs after ratios: \", np.sum(df.isna().sum()))\n",
    "    df = df.replace([np.inf, -np.inf], 999)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38f25e-e2e4-48f3-8cd9-5a7dcad17257",
   "metadata": {},
   "source": [
    "We now separate the feature matrix ($X$) from the target data ($Y$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa2b99-63e4-48f0-a0c5-5d91760fd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'X_fv' \n",
    "\n",
    "# Feature matrix\n",
    "X = df.drop(columns=[target])  \n",
    "feature_names = X.columns\n",
    "\n",
    "# Response vector\n",
    "Y = df[target] \n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721a58c-84ce-4722-9ff4-291a98e58a90",
   "metadata": {},
   "source": [
    "We scale the data to have zero mean and unit variance. This is not required for many ML methods, including random forrest and XGBoost, but it does not hurt to do it and we have seen this leads to better result performances. See the [issue](https://github.com/scikit-learn/scikit-learn/issues/29922#issuecomment-2460276129) in the Scikit-learn documentation for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfad71e-8221-4cb0-869b-721dcb409251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(Z, mode):\n",
    "    # Standard scaler to have zero mean and unit variance\n",
    "    norm = StandardScaler().fit(Z)    \n",
    "    # transform your training data\n",
    "    return norm.transform(Z)\n",
    "\n",
    "X_non_norm = X\n",
    "Y_non_norm = Y\n",
    "\n",
    "# X = normalize(X)\n",
    "# Y = normalize(np.array(Y).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "X = X\n",
    "Y *= 10 ** 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31199e38-5090-445d-bbd6-12aeef4f182c",
   "metadata": {},
   "source": [
    "# Random Forest Model for Regression\n",
    "\n",
    "In this cell, we initialize and train a `RandomForestRegressor` model with 300 trees (`n_estimators=300`) and a maximum depth of 4 (`max_depth=4`).  \n",
    "We fit the model on `X` (features) and `Y` (target variable), then make predictions on `X`.  \n",
    "Finally, we evaluate the model's performance using Root Mean Squared Error (RMSE) and R² Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de981611-edf6-4deb-9dac-e9972792f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(n_estimators=300, \n",
    "                                 random_state=616, \n",
    "                                 max_depth=4, \n",
    "                                 verbose=0)\n",
    "\n",
    "model_rf.fit(X, Y)\n",
    "\n",
    "Y_pred = model_rf.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = mean_squared_error(Y, Y_pred)\n",
    "r2 = r2_score(Y, Y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc2b76-98af-4281-88d6-57c62fc48606",
   "metadata": {},
   "source": [
    "## Cross-Validation -- Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "In this cell, we define a `RandomForestRegressor` model with the `absolute_error` criterion and use `GridSearchCV` to find the best hyperparameters.  \n",
    "We specify a range of values for `max_depth`, `min_samples_leaf`, `min_samples_split`, `max_features`, and `n_estimators`.  \n",
    "`GridSearchCV` performs a 5-fold cross-validation (`cv=5`) to evaluate different parameter combinations, using parallel computation (`n_jobs=-2`) for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e12c0a-e7d4-4ca0-ab2f-07b1ec45b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\n",
    "\n",
    "cv_params = {'max_depth': [4,6,8,10], \n",
    "             'min_samples_leaf': [2,5],\n",
    "             'min_samples_split': [5, 10],\n",
    "             'max_features': [5, 10],\n",
    "             'n_estimators': [100, 200, 400, 800]\n",
    "             }  \n",
    "\n",
    "rf_cv = GridSearchCV(rf, cv_params, cv=5, verbose=1, n_jobs=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048296a6-1f47-4950-8f41-09f30bbb5bd6",
   "metadata": {},
   "source": [
    "## Fit the model - takes ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a407c4-a1ff-43e1-88b9-9164e3416f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_cv.fit(X, Y)\n",
    "rf_cv.best_params_, rf_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e3964-193f-4e9c-9b67-8f3cf29de87f",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We now select the best model and train again with the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7cb98-73f2-4259-a7b9-1167b724e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(n_estimators=rf_cv.best_params_[\"n_estimators\"], \n",
    "                                 random_state=616, \n",
    "                                 max_depth=rf_cv.best_params_[\"max_depth\"], \n",
    "                                 min_samples_leaf=rf_cv.best_params_[\"min_samples_leaf\"],\n",
    "                                 min_samples_split=rf_cv.best_params_[\"min_samples_split\"],\n",
    "                                 criterion=\"absolute_error\",\n",
    "                                 verbose=0)\n",
    "\n",
    "model_rf.fit(X, Y)\n",
    "\n",
    "Y_pred = model_rf.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = mean_squared_error(Y, Y_pred)\n",
    "r2 = r2_score(Y, Y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ddd5c-a5e8-4257-9162-83a1a5ebfb5c",
   "metadata": {},
   "source": [
    "We can visualize the quality of the fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e61fcd-f044-4369-947a-61080773756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xline = np.linspace(0.0, 100, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.scatter(Y, Y_pred, color='blue', alpha=0.4, s=50)\n",
    "plt.plot(xline, xline, color='red', linestyle='--')\n",
    "\n",
    "plt.title(f'Cross-Validation Predictions vs Actual')\n",
    "plt.xlabel(r'Measured $X_{fv}$', fontsize=16)\n",
    "plt.ylabel(r'Predicted $X_{fv}$', fontsize=16)\n",
    "\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,100)\n",
    "# plt.axes().set_aspect('equal')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Random Forest Prediction\", fontsize=16)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"figures/regression_plot.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b07102-841d-4cff-9e6f-8743bf506a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_rf.feature_importances_\n",
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54faaf7-7408-4a9e-be4d-d66da14163cc",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "We extract and analyze the feature importances from the trained `RandomForestRegressor` model.  We use 3 alternative feature importance methods.\n",
    " \n",
    "We store this information in a DataFrame, sort it in descending order, and print the results to identify the most influential features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075f878-7de8-49eb-8d94-bd27c1696826",
   "metadata": {},
   "source": [
    "### I - Gini Index\n",
    "The `feature_importances_` attribute provides the Gini importance scores, which indicate how much each feature contributes to the model's predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4a05f-bb38-437d-852b-45bbe7c483ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_rf.feature_importances_\n",
    "feature_imp_gini = pd.DataFrame({'Feature': feature_names, 'Gini Importance': importances}).sort_values('Gini Importance', ascending=False) \n",
    "print(feature_imp_gini.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c22e76-67fe-4867-b0b9-c5b47a54c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "\n",
    "plt.barh(feature_imp_gini[\"Feature\"].head(50), feature_imp_gini[\"Gini Importance\"].head(50), color='skyblue')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.title('Feature Importance - Gini Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ee3e9-f12c-418e-8113-817d98233e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_gini[\"Order Gini\"] = np.arange(1, feature_imp_gini.shape[0]+1)\n",
    "feature_imp_gini.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20367f92-eff1-48ea-90ab-ac9ef0af6dac",
   "metadata": {},
   "source": [
    "### II - Permutation Test\n",
    "\n",
    "we compute permutation importance for the trained `RandomForestRegressor` model.  \n",
    "Permutation importance measures how much the model’s performance decreases when a feature’s values are randomly shuffled, providing an alternative view of feature importance.  \n",
    "We use the `permutation_importance` function with 10 repeats (`n_repeats=10`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeccd11d-82dd-4337-b1d2-30a0cee20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(model_rf, X, Y, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "\n",
    "feature_imp_perm = pd.DataFrame({'Feature': feature_names, 'Permutation Importance': result.importances_mean}).sort_values('Permutation Importance', ascending=False)\n",
    "# print(feature_imp_perm)\n",
    "\n",
    "feature_imp_perm[\"Order Perm\"] = np.arange(1, feature_imp_perm.shape[0]+1)\n",
    "feature_imp_perm.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c4828-0580-4172-a419-290b34ab66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 12))\n",
    "\n",
    "plt.barh(feature_imp_perm[\"Feature\"], feature_imp_perm[\"Permutation Importance\"], color='skyblue')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.title('Feature Importance - Permutation Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62578b55-647b-4918-8374-0da9b642637e",
   "metadata": {},
   "source": [
    "### III - Shapely values\n",
    "\n",
    "This is a much better method, but we will see it gives very similar results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca54e9-0be6-4676-9a37-f8b21675e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21aab91-9b1f-44f9-a2b7-f251e79f34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Shap values\n",
    "choosen_instance = X.loc[[21]]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], choosen_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970177a-0b36-4660-bbda-ba68ddf31a89",
   "metadata": {},
   "source": [
    "Rather than use a typical feature importance bar chart, we use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c7c27-7da5-4d32-991f-bb4b9dbb1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_instance = X\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416cf13-b5f1-4e78-a357-b6448ca9042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = np.array(X.columns[np.argsort(np.abs(shap_values).mean(0))])[::-1]\n",
    "feature_shap = np.sort(np.abs(shap_values).mean(0))[::-1]\n",
    "feature_imp_shap = pd.DataFrame({\"Feature\": feature_order,\n",
    "                                  \"Shap Value\": feature_shap,\n",
    "                                  \"Order Shap\": np.arange(1, X.shape[1]+1)})\n",
    "feature_imp_shap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40007432-1acd-4e49-ad28-e1dc036adbe6",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Figures in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e93e5-d787-4d19-a977-17452be50b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.merge(feature_imp_gini, feature_imp_perm, how=\"inner\", on=\"Feature\")\n",
    "df_summary = pd.merge(df_summary, feature_imp_shap, how=\"inner\", on=\"Feature\")\n",
    "df_summary[\"Order total\"] = df_summary[\"Order Gini\"] + df_summary[\"Order Perm\"] + df_summary[\"Order Shap\"]\n",
    "df_summary = df_summary.sort_values(by = \"Order total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593d1d5-0091-4061-8ee3-8a8d12f92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df_summary[[\"Feature\", \"Order Gini\", \"Order Perm\", \"Order Shap\"]].set_index(\"Feature\").head(10)\n",
    "df_short = df_short.rename(columns={\"Order Gini\": \"Gini Index\", \"Order Perm\": \"Permutation Index\", \"Order Shap\": \"SHAP Value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ce2e7-11bd-4d94-8bc6-b9b562081f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd052a-4d5f-4df1-b6ea-c97ebfbd0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short\n",
    "ax = sns.heatmap(df_short.transpose(), \n",
    "                 # annot=True, \n",
    "                 linewidth=4.0, \n",
    "                 cmap=sns.color_palette(\"Spectral\", 9), \n",
    "                 square=True, cbar=False, annot_kws={\"size\":14})\n",
    "\n",
    "ax.set_title(\"Feature Importance Rankings\", fontsize=16)\n",
    "ax.set_xlabel(\"Component\", fontsize=14)\n",
    "# ax.set_ylabel(\"Criteria\", fontsize=14)\n",
    "\n",
    "ax.set_yticklabels([\"Gini\\nIndex\", \"Permutation\\nIndex\", \"SHAP\\nValue\"], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"figures/rankings.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd8b80-3165-4af2-9b2e-9011f8a9bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46907c5-74a0-4bfe-9846-5f319831bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = df_summary[[\"Feature\", \"Gini Importance\", \"Permutation Importance\", \"Shap Value\"]].set_index(\"Feature\")\n",
    "df_values[\"Gini Importance (norm)\"] = df_values[\"Gini Importance\"] / np.sum(df_values[\"Gini Importance\"])\n",
    "df_values[\"Permutation Importance (norm)\"] = df_values[\"Permutation Importance\"] / np.sum(df_values[\"Permutation Importance\"])\n",
    "df_values[\"Shap Value (norm)\"] = df_values[\"Shap Value\"] / np.sum(df_values[\"Shap Value\"])\n",
    "\n",
    "df_values.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d239a9-64d2-49eb-870a-5ea9194028e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286e16c-b51a-4467-b258-cf69a4bbc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_ranking = plt.subplots(figsize=(7,10))\n",
    "\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "plt.grid(axis = 'x', linestyle = '--', linewidth = 0.5)\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.scatter(df_values[\"Gini Importance (norm)\"], np.arange(df_values.shape[0]+1, 1, -1), label=\"Gini Importance\", marker=\"D\", c=\"#c0392b\", s=50, zorder = 100)\n",
    "plt.scatter(df_values[\"Permutation Importance (norm)\"], np.arange(df_values.shape[0]+1, 1, -1), label=\"Permutation Importance\", marker=\"o\", c=\"#27ae60\", s=50, zorder = 100)\n",
    "plt.scatter(df_values[\"Shap Value (norm)\"], np.arange(df_values.shape[0]+1, 1, -1), label=\"Shap Value\", marker=\"s\", c=\"#8e44ad\", s=50, zorder = 100)\n",
    "\n",
    "plt.xlabel(\"Normalized index\", fontsize=16)\n",
    "plt.ylabel(\"Components\", fontsize=16)\n",
    "ax_ranking.set_yticks(np.arange(df_values.shape[0]+1, 1, -1), df_values.index)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.xlim(0,0.18)\n",
    "plt.ylim(28.4, 53.5)\n",
    "plt.title(\"Feature Importance Ranking\", fontsize=16)\n",
    "\n",
    "leg = plt.legend(title=\"Feature Importance Index\", loc=\"lower right\", fontsize=16, title_fontsize=16)\n",
    "\n",
    "# plt.savefig(\"figures/importance.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7691994-bb12-4c91-91f7-b0ddf245e019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
